---
title: "Meta-analysis: The Impact of Data Collection Methods on the Efficiency of Playback Experiments"
author: "2575823"
format: html
editor: visual
bibliography: references.bib
---

# Introduction

## **Overview of Playback Experiments**[@noad2017][@derosa2021]

Playback experiments are a cornerstone of behavioral research, enabling controlled manipulation of sensory stimuli to investigate animal communication, cognition, and social behavior. By presenting recorded signals—acoustic, visual, or multimodal—to subjects, researchers can infer the function and meaning of signals, test hypotheses about perception and cognition, and assess behavioral responses in both natural and controlled environments. Applications span avian behavior, primate communication, predator recognition, and human-computer interaction, underscoring their versatility and significance in experimental biology and psychology.

## **Rationale for Comparing Data Collection Methods**[@suri2025][@mercermoss2015][@fischer2013]

The choice of data collection method in playback experiments is pivotal, influencing not only the quality and reliability of the data but also the efficiency and validity of the experimental outcomes. It is necessary to design experiments based on previous research because:

-   Data quality and time efficiency are directly affected by the method and platform used.

-   Methodological rigor (e.g., protocol adherence, observer blinding) determines the validity and reproducibility of findings.

-   Technological advances and the rise of online/automated platforms introduce new opportunities and challenges for data collection.

```{r}
# Maybe I want to concentrated on acoustic experiment on birds。。
```

......However, while individual studies often acknowledge these methodological constraints, there has been no systematic synthesis quantifying how data collection methods impact the overall efficiency of playback-based research in birds.

This meta-analysis aims to address this gap by evaluating how different data collection approaches influence the outcomes, reliability, and interpretability of avian acoustic playback experiments. By comparing methodological features across published studies, we assess which practices enhance experimental efficiency and which introduce predictable sources of variation. Our findings provide a foundation for improving experimental design, increasing reproducibility, and guiding best practices for future research in avian bioacoustics.

```{r}
# next time: connect the content
```

# Method[@odea2021][@mathot2024]

## Meta-analysis Process

### Literature search and screening

1.  Follow the PRISMA guidelines: Clearly report the process of literature screening (searching, deduplication, screening, and inclusion).

2.  Database: Select core databases such as Web of Science (WOS) and Scopus.

3.  Search Terms: Design three sets of search terms to ensure comprehensiveness and relevance.

    For example, ["playback" OR "acoustic stimulation" OR "song broadcast" OR "vocal mimicry"]{.underline}

### Inclusion/Exclusion Criteria

1.  It must include an acoustic playback as the experimental treatment.

2.  A control group (such as silence, white noise, or non-threatening sounds) must be reported.

3.  Mean, standard deviations (SD), or standard errors, as well as sample sizes (N), must be provided.

4.  It must be birds.

5.  It must be clearly described in the method section how the response data was collected.

### Data coding and Extraction

1.  Basic information: Extract author, year, journal, and species.

2.  Effect size data: Extract the Mean, SD, and N values for the experimental group and the control group.

3.  Key Moderator: "Data Collection Method".

    Example of encoding:

    Observer_Presence: (A) In-person observation, (B) Remote observation (e.g., via video camera), (C) Fully automated observation (e.g., using acoustic sensor).

    Response_Type: (A) Behavioural Response, (B) Physiological Response, (C) Life History Response (This classification is borrowed from Mathot).

4.  Other Putative Moderators: Extract other factors that might influence the outcome.

    Setting: (A) Laboratory, (B) Field.

    Control_Type: (A) Silence/Blank , (B) Disturbance Control (e.g., speaker presence), (C) Non-target Species Sound.

    Duration: Experiment duration.

### Effect Size Calculation

1.  Effect size selection: Use the standardised mean difference, which is also known as Hedge's g.

2.  Calculation: Use the escalc function in the metafor package to calculate Hedge's g and its sampling variance.

3.  Directional coding: Ensure that all effect sizes have the same direction. For example, uniformly code as "Response to Playback is enhanced" as a positive value.

### Meta-analysis

1.  Model selection: Utilize the multilevel Meta-Analysis model , using the rma.mv function from the metafor package in R.

2.  Handling Non-Independence.

3.  Calculate the total heterogeneity ($I^2$) to assess the extent of differences in effect sizes among the studies.

4.  The "data collection method" was incorporated as a [fixed effect]{.underline} into the model to test whether there were significant differences in the mean effect size (mean SMD) among different methods.

5.  Heteroscedasticity Analysis

### Publication Bias

1.  Funnel Plot: Visual inspection for symmetry.

2.  Egger's Regression: Quantitative Test for the Asymmetry of Funnel Plot.

3.  Time lag effect: To test whether the effect size varies with the publication year.

## Shiny App

# Results and Discussions

## Results

The ... effect sizes finally included in the report (from ... studies, involving ... species)

The overall average effect size (SMD) and its 95% CI of the Playback experiment.

The overall $I^2$ (likely very high)

Main findings (mean value): Whether the data collection method significantly affects the average effect size?

Main findings (variance): Whether the heteroscedastic model is significantly better than the homoscedastic model? (Reported LRT results 53). If so, show the size of heterogeneity (variance) for each group of "data collection methods".

Report other significant covariates (such as: Lab vs. Field).

Report the funnel plot and Egger's regression results.

## Discussion

Explain the main findings.

Limitations

Future

# Conclusion

(Reiterating the core contribution of the research)

This study is the first to quantify the impact of the data collection method itself on the mean and the variance of the results of the Playback experiment.

The study shows that the choice of methodology is an important and overlooked source of heterogeneity among ecological studies.

# AI Disclosure

# Reference
