---
title: "Meta-analysis: The Impact of Data Collection Methods on the Efficiency of Playback Experiments"
author: "2575823"
format: html
editor: visual
bibliography: references.bib
---

# Introduction

## The Predicament of Playback Experiment

Playback experiments are foundational in ecological and behavioral research, enabling scientists to probe animal communication, cognition, and social interactions by presenting controlled auditory or visual stimuli and observing responses. These experiments have been instrumental in advancing our understanding of species-specific behaviors, predator-prey dynamics, and the evolution of communication systems. The methodological rigor and reproducibility of playback experiments are critical, as subtle variations in experimental design can profoundly influence the validity and generalizability of findings. As such, playback experiments are widely regarded as a gold standard for hypothesis testing in field and laboratory settings, underpinning a substantial body of ecological and behavioral literature.

A central challenge in playback research is the pronounced heterogeneity introduced by methodological choices—particularly the mode of data collection. In-person observation, remote video (live or prerecorded), and automated sensor-based approaches each offer distinct advantages and limitations. For instance, remote methods can facilitate larger and more diverse samples, but may compromise the richness of non-verbal cues and introduce new sources of bias. Automated sensors can enhance data accuracy and reduce human error, yet may lack contextual nuance. These differences are not trivial: they can lead to substantial variability in data volume, participant engagement, error rates, and ultimately, experimental outcomes. The resulting heterogeneity complicates the synthesis of findings across studies, as variations in experimental settings, response types, and data collection modalities can yield divergent or even conflicting results

## Solution: A universal interactive analysis tool

Given the proliferation of playback studies employing diverse methodologies, researchers face an urgent need to synthesize these heterogeneous and sometimes contradictory results. Meta-analysis has emerged as the gold standard for evidence integration, offering a systematic framework to quantify effect sizes, assess heterogeneity, and draw robust conclusions from disparate datasets. However, the execution of meta-analyses in ecology is often hampered by steep technical requirements: advanced statistical knowledge, proficiency in programming, and familiarity with complex data structures are typically prerequisites. These barriers can impede ecologists—especially those without specialized quantitative training—from rapidly and reliably synthesizing evidence from the literature.

To address this gap, we present a general-purpose, interactive Shiny app designed to democratize meta-analysis for ecological researchers. By enabling users to upload curated datasets and perform comprehensive analyses—including fixed and random effects models, forest plots, and funnel plots—without the need for advanced programming skills, this tool lowers the technical threshold for evidence synthesis. In this study, we illustrate the app’s functionality and value through the lens of playback experiments, specifically examining how data collection methods influence experimental outcomes. This approach not only showcases the practical utility of interactive meta-analytic tools but also highlights their potential to advance methodological rigor and transparency in ecological research.

# Method[@odea2021][@mathot2024]

## Meta-analysis Process

### Literature search and screening

1.  Follow the PRISMA guidelines: Clearly report the process of literature screening (searching, deduplication, screening, and inclusion).

2.  Database: Select core databases such as Web of Science (WOS) and Scopus.

3.  Search Terms: Design three sets of search terms to ensure comprehensiveness and relevance.

    For example, ["playback" OR "acoustic stimulation" OR "song broadcast" OR "vocal mimicry"]{.underline}

### Inclusion/Exclusion Criteria

1.  It must include an acoustic playback as the experimental treatment.

2.  A control group (such as silence, white noise, or non-threatening sounds) must be reported.

3.  Mean, standard deviations (SD), or standard errors, as well as sample sizes (N), must be provided.

4.  It must be birds.

5.  It must be clearly described in the method section how the response data was collected.

### Data coding and Extraction

1.  Basic information: Extract author, year, journal, and species.

2.  Effect size data: Extract the Mean, SD, and N values for the experimental group and the control group.

3.  Key Moderator: "Data Collection Method".

    Example of encoding:

    Observer_Presence: (A) In-person observation, (B) Remote observation (e.g., via video camera), (C) Fully automated observation (e.g., using acoustic sensor).

    Response_Type: (A) Behavioural Response, (B) Physiological Response, (C) Life History Response (This classification is borrowed from Mathot).

4.  Other Putative Moderators: Extract other factors that might influence the outcome.

    Setting: (A) Laboratory, (B) Field.

    Control_Type: (A) Silence/Blank , (B) Disturbance Control (e.g., speaker presence), (C) Non-target Species Sound.

    Duration: Experiment duration.

### Effect Size Calculation

1.  Effect size selection: Use the standardised mean difference, which is also known as Hedge's g.

2.  Calculation: Use the escalc function in the metafor package to calculate Hedge's g and its sampling variance.

3.  Directional coding: Ensure that all effect sizes have the same direction. For example, uniformly code as "Response to Playback is enhanced" as a positive value.

### Meta-analysis

1.  Model selection: Utilize the multilevel Meta-Analysis model , using the rma.mv function from the metafor package in R.

2.  Handling Non-Independence.

3.  Calculate the total heterogeneity ($I^2$) to assess the extent of differences in effect sizes among the studies.

4.  The "data collection method" was incorporated as a [fixed effect]{.underline} into the model to test whether there were significant differences in the mean effect size (mean SMD) among different methods.

5.  Heteroscedasticity Analysis

### Publication Bias

1.  Funnel Plot: Visual inspection for symmetry.

2.  Egger's Regression: Quantitative Test for the Asymmetry of Funnel Plot.

3.  Time lag effect: To test whether the effect size varies with the publication year.

## Shiny App

# Results and Discussions

## Results

The ... effect sizes finally included in the report (from ... studies, involving ... species)

The overall average effect size (SMD) and its 95% CI of the Playback experiment.

The overall $I^2$ (likely very high)

Main findings (mean value): Whether the data collection method significantly affects the average effect size?

Main findings (variance): Whether the heteroscedastic model is significantly better than the homoscedastic model? (Reported LRT results 53). If so, show the size of heterogeneity (variance) for each group of "data collection methods".

Report other significant covariates (such as: Lab vs. Field).

Report the funnel plot and Egger's regression results.

## Discussion

Explain the main findings.

Limitations

Future

# Conclusion

(Reiterating the core contribution of the research)

This study is the first to quantify the impact of the data collection method itself on the mean and the variance of the results of the Playback experiment.

The study shows that the choice of methodology is an important and overlooked source of heterogeneity among ecological studies.

# AI Disclosure

# Reference
